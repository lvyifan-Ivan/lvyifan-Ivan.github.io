# 对比学习

[对比学习综述](https://zhuanlan.zhihu.com/p/346686467)

## 【对比式学习】

对比式学习着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处

与生成式学习比较，对比式学习不需要关注实例上繁琐的细节，只需要在抽象语义级别的特征空间上学会对数据的区分即可，因此模型以及其优化变得更加简单，且泛化能力更强

<img title="" src="file:///D:/Lyuyifan-Ivan.github.io/docs/多模态NER/img/v2-3af2ec617b3534ef26336fe9866f402a_r.jpg" alt="" data-align="center" width="549">

## 【聚类思想】

缩小与正样本间的距离，扩大与负样本间的距离，使正样本与锚点的距离远远小于负样本与锚点的距离，（或使正样本与锚点的相似度远远大于负样本与锚点的相似度），从而达到他们之间原有空间分布的真实距离

- 丈量二者距离：欧几里得距离，余弦相似度，马氏距离（没人试过，但原理是一样的）

- 目标：给定锚点，通过空间变换，使得锚点与正样本间距离尽可能小，与负样本距离尽可能大

<img title="" src="file:///D:/Lyuyifan-Ivan.github.io/docs/多模态NER/img/屏幕截图%202022-07-11%20162457.jpg" alt="" width="359" data-align="center">

## 【对比思想】

动机：人类不仅能从积极的信号中学习，还能从纠正不良行为中获益

对比学习其实是无监督学习的一种范式

<img title="" src="file:///D:/Lyuyifan-Ivan.github.io/docs/多模态NER/img/v2-a17404a49d4f980ac69653464dbcc3fb_r.jpg" alt="" data-align="center" width="470">

## 【对比损失】



## 【基础论文】

### Representation Learning with Contrastive Predictive Coding

[论文链接](https://arxiv.org/abs/1807.03748)

[代码链接](https://github.com/davidtellez/contrastive-predictive-coding)

很多时候，很多数据**维度高、label相对少**，我们并不希望浪费掉没有label的那部分data。所以在label少的时候，可以利用无监督学习帮助我们学到数据本身的高级信息，从而对下游任务有很大的帮助

**提出方法**

- 将高维数据压缩到更紧凑的隐空间中，在其中条件预测更容易建模
- 用自回归模型在隐空间中预测未来步骤
- 依靠NCE来计算损失函数（和学习词嵌入方式类似），从而可以对整个模型进行端到端的训练
- 对于多模态的数据有可以学到高级信息

<img src="file:///D:/Lyuyifan-Ivan.github.io/docs/多模态NER/img/v2-2192d04513f5f5b6fc7928d94cb3a72b_r.jpg" title="" alt="" data-align="center">


