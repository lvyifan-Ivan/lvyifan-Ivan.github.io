# 百度飞浆-产品评论观点提取

# 队伍名-片上超算

<img src="D:\Lyuyifan-Ivan.github.io\docs\image\屏幕截图 2022-09-25 154051.jpg" alt="屏幕截图 2022-09-25 154051" style="zoom: 33%;" />

## 比赛任务

本赛题旨在从非结构化的评论文本中提取标准化、结构化的信息，如产品名、评论维度、评论观点等。此处希望大家能够通过自然语言处理的语义情感分析技术判断出一段银行产品评论文本的情感倾向，并能进一步通过语义分析和实体识别技术，标识出评论所讨论的产品名，评价指标和评价关键词。

## 比赛背景

该比赛为长期比赛，原题目来自于2021年CCF大数据与计算智能大赛金融道题题目《产品评论观点提取》



## 数据说明

|   实体label    |             表示说明             |
| :------------: | :------------------------------: |
|     B-BANK     |        代表银行实体的开始        |
|     I-BANK     |        代表银行实体的内部        |
|   B-PRODUCT    |        代表产品实体的开始        |
|   I-PRODUCT    |        代表产品实体的内部        |
|  B-COMMENTS_N  |       代表用户评论（名词）       |
|  I-COMMENTS_N  |  代表用户评论（名词）实体的内部  |
| B-COMMENTS_ADJ |      代表用户评论（形容词）      |
| I-COMMENTS_ADJ | 代表用户评论（形容词）实体的内部 |
|       O        |       代表不属于标注的范围       |



#### 实体标注示例

<img src="D:\Lyuyifan-Ivan.github.io\docs\image\屏幕截图 2022-09-25 161151.jpg" alt="屏幕截图 2022-09-25 161151" style="zoom:50%;" />

## 评分方式

本次评测采用的评价评价指标 S = 0.5S1+0.5S2

本次挑战一部分为NER任务，S1以strict-F1作为衡量标准。

## 部分代码分享

模型结构使用bert-crf、roberta-crf，roberta由于训练数据集更大和优化了预训练模型结构，效果更好

也尝试使用hugging-face提供的BertForTokenClassification

```python
# BERT_CRF_Model
from paddlenlp.transformers import BertForTokenClassification, BertModel
from paddlenlp.layers import LinearChainCrf, LinearChainCrfLoss, ViterbiDecoder
from paddle.nn import Linear, Dropout, LSTM

class BERT_CRF_Model(paddle.nn.Layer):
    def __init__(self, bert_name, input_size, hidden_size, label_num, crf_lr):
        super().__init__()
        self.bert_name = bert_name
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.model = BertModel.from_pretrained(self.bert_name)
        self.dropout = Dropout(p=0.5)
        self.lstm = LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=1, direction="bidirectional", dropout=0.5)
        self.layer = Linear(in_features=self.hidden_size*2, out_features=label_num)
        self.crf = LinearChainCrf(num_labels=label_num, crf_lr=crf_lr, with_start_stop_tag=False)
        self.crf_loss = LinearChainCrfLoss(self.crf)
        self.decoder = ViterbiDecoder(self.crf.transitions, with_start_stop_tag=False)
    def forward(self, input_ids, token_type_ids=None, length=None, labels=None):
        sequence_output, _ = self.model(input_ids, token_type_ids)
        logits = self.dropout(sequence_output)
        logits = self.lstm(logits)
        logits = self.layer(logits)
        if labels is not None:
            loss = self.crf_loss(logits, length, labels)
            return loss
        else:
            scores, pred = self.decoder(logits, length)
            return pred
```

```python
from paddlenlp.transformers import BertTokenizer, BertForTokenClassification
from paddlenlp.transformers import RobertaTokenizer, RobertaChineseTokenizer, RobertaForTokenClassification
from paddlenlp.transformers import ErnieTokenizer, ErnieForTokenClassification
from paddlenlp.transformers import ElectraTokenizer, ElectraForTokenClassification

def make_NERModel(model_name, label_num, input_size=None, hidden_size=None):
    bert_name = "bert-base-chinese" # bert-base-chinese、bert-wwm-chinese、bert-wwm-ext-chinese
    roberta_name = "roberta-wwm-ext-large" # roberta-wwm-ext-large、roberta-wwm-ext
    ernie_name = "ernie-1.0" 
    electra_name = "chinese-electra-base"
    if model_name == "bert_base":
        tokenizer = BertTokenizer.from_pretrained(bert_name)
        model = BertForTokenClassification.from_pretrained(bert_name, num_classes=label_num, dropout=0.1)
    elif model_name == "bert_crf":
        tokenizer = BertTokenizer.from_pretrained(bert_name)
        model = BERT_CRF_Model(bert_name, input_size, hidden_size, label_num, crf_lr=100)
    elif model_name == "roberta_base":
        tokenizer = RobertaTokenizer.from_pretrained(roberta_name)
        model = RobertaForTokenClassification.from_pretrained(roberta_name, num_classes=label_num, dropout=0.1)
    elif model_name == "roberta_crf":
        tokenizer = RobertaTokenizer.from_pretrained(roberta_name)
        # tokenizer = RobertaChineseTokenizer.from_pretrained(roberta_name)
        model = RoBERTa_CRF_Model(roberta_name, input_size, hidden_size, label_num, crf_lr=100)
    elif model_name == "ernie_base":
        tokenizer = ErnieTokenizer.from_pretrained(ernie_name)
        model = ErnieForTokenClassification.from_pretrained(ernie_name, num_classes=label_num, dropout=0.5)
    elif model_name == "electra_base":
        tokenizer = ElectraTokenizer.from_pretrained(electra_name)
        model = ElectraForTokenClassification.from_pretrained(electra_name, num_classes=label_num)
    else:
        raise RuntimeError("Load Model error :%s"%(model_name))
    return model, tokenizer
```

